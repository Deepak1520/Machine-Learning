{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Practical 1 - Linear Regression\n",
    "\n",
    "Names: {Deepak Budha, Shyam Yadav, }  \n",
    "Summer Term 2024   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides you with the assignments and the overall code structure you need to complete the assignment. There are also questions that you need to answer in text form. Please use full sentences and reasonably correct spelling/grammar.\n",
    "\n",
    "Regarding submission & grading:\n",
    "\n",
    "- Work in groups of three and hand in your solution as a group.\n",
    "\n",
    "- Solutions need to be uploaded to StudIP until the submission date indicated in the course plan. Please upload a copy of this notebook and a PDF version of it after you ran it.\n",
    "\n",
    "- Solutions need to be presented to tutors in tutorial. Presentation dates are listed in the course plan. Every group member needs to be able to explain everything.\n",
    "\n",
    "- You have to solve N-1 practicals to get admission to the exam.\n",
    "\n",
    "- For plots you create yourself, all axes must be labeled. \n",
    "\n",
    "- Do not change the function interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Jupyter Notebook provides the possibility of using libraries, functions and variables globally. This means, once you import the libraries, functions, etc. you won't have to import them again in the next cell. However, if for any reason you end the session (crash, timeout, etc.), then you'll have to run this cell to have your libraries imported again. So, let's go ahead and import whatever we need in this homework assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:36:43.107Z",
     "iopub.status.busy": "2020-05-28T07:36:43.081Z",
     "iopub.status.idle": "2020-05-28T07:36:44.905Z",
     "shell.execute_reply": "2020-05-28T07:36:44.989Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The  dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of over 20.000 materials and lists their physical features. From these features, we want to learn how to predict the critical temperature, i.e. the temperature we need to cool the material to so it becomes superconductive. First load and familiarize yourself with the data set a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:36:44.962Z",
     "iopub.status.busy": "2020-05-28T07:36:44.940Z",
     "iopub.status.idle": "2020-05-28T07:36:45.441Z",
     "shell.execute_reply": "2020-05-28T07:36:46.398Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/superconduct_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/superconduct_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/superconduct_train.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/superconduct_train.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:36:45.512Z",
     "iopub.status.busy": "2020-05-28T07:36:45.480Z",
     "iopub.status.idle": "2020-05-28T07:36:45.578Z",
     "shell.execute_reply": "2020-05-28T07:36:46.416Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is rather large, we prepare a small subset of the data as training set, and another subset as test set. To make the computations reproducible, we set the random seed. This makes the train and test splits same even if you re-run the notebook. Keeping the splits same is important for the fair models comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:36:45.647Z",
     "iopub.status.busy": "2020-05-28T07:36:45.626Z",
     "iopub.status.idle": "2020-05-28T07:36:45.677Z",
     "shell.execute_reply": "2020-05-28T07:36:46.433Z"
    }
   },
   "outputs": [],
   "source": [
    "target_clm = 'critical_temp'  # the critical temperature is our target variable\n",
    "n_trainset = 200  # size of the training set\n",
    "n_testset = 500  # size of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:36:45.726Z",
     "iopub.status.busy": "2020-05-28T07:36:45.707Z",
     "iopub.status.idle": "2020-05-28T07:36:45.776Z",
     "shell.execute_reply": "2020-05-28T07:36:46.444Z"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed to make sure every test set is the same\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "idx = np.arange(data.shape[0])\n",
    "idx_shuffled = np.random.permutation(idx)  # shuffle indices to split into training and test set\n",
    "\n",
    "test_idx = idx_shuffled[:n_testset]\n",
    "train_idx = idx_shuffled[n_testset:n_testset+n_trainset]\n",
    "train_full_idx = idx_shuffled[n_testset:]\n",
    "\n",
    "X_test = data.loc[test_idx, data.columns != target_clm].values\n",
    "y_test = data.loc[test_idx, data.columns == target_clm].values\n",
    "print('Test set shapes (X and y)', X_test.shape, y_test.shape)\n",
    "\n",
    "X_train = data.loc[train_idx, data.columns != target_clm].values\n",
    "y_train = data.loc[train_idx, data.columns == target_clm].values\n",
    "print('Small training set shapes (X and y):', X_train.shape, y_train.shape)\n",
    "\n",
    "X_train_full = data.loc[train_full_idx, data.columns != target_clm].values\n",
    "y_train_full = data.loc[train_full_idx, data.columns == target_clm].values\n",
    "print('Full training set shapes (X and y):', X_train_full.shape, y_train_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Task 1: Plot the dataset\n",
    "\n",
    "To explore the dataset, use `X_train_full` and `y_train_full` for two descriptive plots:\n",
    "\n",
    "* **Histogram** of the target variable. Use `plt.hist`.\n",
    "\n",
    "* **Scatter plots** relating the target variable to one of the feature values. For this you will need 81 scatter plots. Arrange them in one big figure with 9x9 subplots. Use `plt.scatter`. You may need to adjust the marker size and the alpha blending value. \n",
    "\n",
    "Furthermore, we need to normalize the data, such that each feature has a mean of zero mean and a variance of one. Implement a function `normalize` which normalizes the data. Print the means and standard variation of the first five features before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:36:45.829Z",
     "iopub.status.busy": "2020-05-28T07:36:45.809Z",
     "iopub.status.idle": "2020-05-28T07:36:45.893Z",
     "shell.execute_reply": "2020-05-28T07:36:46.455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Histogram of the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:36:45.936Z",
     "iopub.status.busy": "2020-05-28T07:36:45.919Z",
     "iopub.status.idle": "2020-05-28T07:37:00.826Z",
     "shell.execute_reply": "2020-05-28T07:37:00.906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plots of the target variable vs. features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which material properties may be useful for predicting superconductivity? What other observations can you make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  Implement your own OLS estimator\n",
    "\n",
    "We want to use linear regression to predict the critical temperature. Implement the ordinary least squares estimator without regularization 'by hand':\n",
    "\n",
    "$w = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "To make life a bit easier, we provide a function that can be used to plot regression results. In addition it computes the mean squared error and the squared correlation between the true and predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:01.042Z",
     "iopub.status.busy": "2020-05-28T07:37:00.996Z",
     "iopub.status.idle": "2020-05-28T07:37:01.095Z",
     "shell.execute_reply": "2020-05-28T07:37:01.193Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_regression_results(y_test, y_pred, weights):\n",
    "    '''Produces three plots to analyze the results of linear regression:\n",
    "        -True vs predicted\n",
    "        -Raw residual histogram\n",
    "        -Weight histogram\n",
    "\n",
    "    Inputs:\n",
    "        y_test: (n_observations,) numpy array with true values\n",
    "        y_pred: (n_observations,) numpy array with predicted values\n",
    "        weights: (n_weights) numpy array with regression weights'''\n",
    "\n",
    "    print('MSE: ', mean_squared_error(y_test, y_pred))\n",
    "    print('r^2: ', r2_score(y_test, y_pred))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    # predicted vs true\n",
    "    ax[0].scatter(y_test, y_pred, s=2)\n",
    "    ax[0].set_title('True vs. Predicted')\n",
    "    ax[0].set_xlabel('True %s' % (target_clm))\n",
    "    ax[0].set_ylabel('Predicted %s' % (target_clm))\n",
    "\n",
    "    # residuals\n",
    "    error = np.squeeze(np.array(y_test)) - np.squeeze(np.array(y_pred))\n",
    "    ax[1].hist(np.array(error), bins=30)\n",
    "    ax[1].set_title('Raw residuals')\n",
    "    ax[1].set_xlabel('(true-predicted)')\n",
    "\n",
    "    # weight histogram\n",
    "    ax[2].hist(weights, bins=30)\n",
    "    ax[2].set_title('weight histogram')\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we here show you how to use this function with random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:01.171Z",
     "iopub.status.busy": "2020-05-28T07:37:01.129Z",
     "iopub.status.idle": "2020-05-28T07:37:02.128Z",
     "shell.execute_reply": "2020-05-28T07:37:02.403Z"
    }
   },
   "outputs": [],
   "source": [
    "# weights is a vector of length 82: the first value is the intercept (beta0), then 81 coefficients\n",
    "weights = np.random.randn(82)\n",
    "\n",
    "# Model predictions on the test set\n",
    "y_pred_testing = np.random.randn(y_test.size) * np.max(y_test)\n",
    "\n",
    "plot_regression_results(y_test, y_pred_testing, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement OLS linear regression yourself. Use `X_train` and `y_train` for estimating the weights and compute the MSE and $r^2$ from `X_test`. When you call our plotting function with the regression result, you should get mean squared error of 707.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:02.216Z",
     "iopub.status.busy": "2020-05-28T07:37:02.181Z",
     "iopub.status.idle": "2020-05-28T07:37:02.273Z",
     "shell.execute_reply": "2020-05-28T07:37:02.424Z"
    }
   },
   "outputs": [],
   "source": [
    "def ols_regression(X_test, X_train, y_train):\n",
    "    '''Computes OLS weights for linear regression without regularization on the training set and\n",
    "       returns weights and testset predictions.\n",
    "\n",
    "       Inputs:\n",
    "         X_test: (n_observations, 81), numpy array with predictor values of the test set\n",
    "         X_train: (n_observations, 81), numpy array with predictor values of the training set\n",
    "         y_train: (n_observations,) numpy array with true target values for the training set\n",
    "\n",
    "       Outputs:\n",
    "         weights: The weight vector for the regerssion model including the offset\n",
    "         y_pred: The predictions on the TEST set\n",
    "\n",
    "       Note:\n",
    "         Both the training and the test set need to be appended manually by a columns of 1s to add\n",
    "         an offset term to the linear regression model.\n",
    "    '''\n",
    "\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    return weights, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:02.350Z",
     "iopub.status.busy": "2020-05-28T07:37:02.322Z",
     "iopub.status.idle": "2020-05-28T07:37:02.989Z",
     "shell.execute_reply": "2020-05-28T07:37:03.266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plots of the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Is the linear regression model good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Compare your implementation to sklearn\n",
    "\n",
    "Now, familiarize yourself with the sklearn library. In the section on linear models:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "\n",
    "you will find `sklearn.linear_model.LinearRegression`, the `sklearn` implementation of the OLS estimator. Use this sklearn class to implement OLS linear regression. Again obtain estimates of the weights on `X_train` and `y_train` and compute the MSE and $r^2$ on `X_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:03.074Z",
     "iopub.status.busy": "2020-05-28T07:37:03.042Z",
     "iopub.status.idle": "2020-05-28T07:37:03.124Z",
     "shell.execute_reply": "2020-05-28T07:37:03.285Z"
    }
   },
   "outputs": [],
   "source": [
    "def sklearn_regression(X_test, X_train, y_train):\n",
    "    '''Computes OLS weights for linear regression without regularization using the sklearn library on the training set and\n",
    "       returns weights and testset predictions.\n",
    "\n",
    "       Inputs:\n",
    "         X_test: (n_observations, 81), numpy array with predictor values of the test set\n",
    "         X_train: (n_observations, 81), numpy array with predictor values of the training set\n",
    "         y_train: (n_observations,) numpy array with true target values for the training set\n",
    "\n",
    "       Outputs:\n",
    "         weights: The weight vector for the regerssion model including the offset\n",
    "         y_pred: The predictions on the TEST set\n",
    "\n",
    "       Note:\n",
    "         The sklearn library automatically takes care of adding a column for the offset.\n",
    "    '''\n",
    "\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    return weights, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:03.206Z",
     "iopub.status.busy": "2020-05-28T07:37:03.180Z",
     "iopub.status.idle": "2020-05-28T07:37:04.059Z",
     "shell.execute_reply": "2020-05-28T07:37:04.195Z"
    }
   },
   "outputs": [],
   "source": [
    "weights, y_pred = sklearn_regression(X_test, X_train, y_train)\n",
    "plot_regression_results(y_test, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented everything correctly, the MSE is again 707.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the larger training set, `X_train_full` and `y_train_full`, and again evaluate on `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:04.148Z",
     "iopub.status.busy": "2020-05-28T07:37:04.114Z",
     "iopub.status.idle": "2020-05-28T07:37:05.126Z",
     "shell.execute_reply": "2020-05-28T07:37:05.554Z"
    }
   },
   "outputs": [],
   "source": [
    "weights, y_pred = sklearn_regression(X_test, X_train_full, y_train_full)\n",
    "plot_regression_results(y_test, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How does test set performance change? What else changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOU ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Regularization with ridge regression\n",
    "\n",
    "We will now explore how a penalty term on the weights can improve the prediction quality for finite data sets. Implement the analytical solution of ridge regression \n",
    "\n",
    "$w = (X^TX + \\alpha I_D)^{-1}X^Ty$\n",
    "\n",
    "\n",
    "as a function that can take different values of $\\alpha$, the regularization strength, as an input. In the lecture, this parameter was called $\\lambda$, but this is a reserved keyword in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:05.219Z",
     "iopub.status.busy": "2020-05-28T07:37:05.181Z",
     "iopub.status.idle": "2020-05-28T07:37:05.275Z",
     "shell.execute_reply": "2020-05-28T07:37:05.574Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_regression(X_test, X_train, y_train, alpha):\n",
    "    '''Computes OLS weights for regularized linear regression with regularization strength alpha\n",
    "       on the training set and returns weights and testset predictions.\n",
    "\n",
    "       Inputs:\n",
    "         X_test: (n_observations, 81), numpy array with predictor values of the test set\n",
    "         X_train: (n_observations, 81), numpy array with predictor values of the training set\n",
    "         y_train: (n_observations,) numpy array with true target values for the training set\n",
    "         alpha: scalar, regularization strength\n",
    "\n",
    "       Outputs:\n",
    "         weights: The weight vector for the regression model including the offset\n",
    "         y_pred: The predictions on the TEST set\n",
    "\n",
    "       Note:\n",
    "         Both the training and the test set need to be appended manually by a columns of 1s to add\n",
    "         an offset term to the linear regression model.\n",
    "    '''\n",
    "\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    return weights, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the ridge regression on `X_train` with an alpha value of 10 and plot the obtained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ridge regression with alpha=10\n",
    "\n",
    "\n",
    "# Plot regression results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test a range of log-spaced $\\alpha\\text{s}$ (~10-20), which cover several orders of magnitude, e.g. from 10^-7 to 10^7. \n",
    "\n",
    "* For each $\\alpha$, you will get one model with one set of weights. \n",
    "* For each model, compute the error on the test set. \n",
    "\n",
    "Store both the errors and weights of all models for later use. You can use the function `mean_squared_error` from sklearn (imported above) to compute the MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:05.363Z",
     "iopub.status.busy": "2020-05-28T07:37:05.331Z",
     "iopub.status.idle": "2020-05-28T07:37:05.408Z",
     "shell.execute_reply": "2020-05-28T07:37:05.587Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-7, 7, 20)\n",
    "\n",
    "# ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- END CODE -------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a single plot that shows for each coefficient how it changes with $\\alpha$, i.e. one line per coefficient. Also think about which scale is appropriate for your $\\alpha$-axis. You can set this using `plt.xscale(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:05.488Z",
     "iopub.status.busy": "2020-05-28T07:37:05.458Z",
     "iopub.status.idle": "2020-05-28T07:37:06.656Z",
     "shell.execute_reply": "2020-05-28T07:37:06.776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot of coefficients vs. alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the values of the weights largest on the left? Do they all change monotonically? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot how the performance (i.e. the error) changes as a function of $\\alpha$. As a sanity check, the MSE value for very small $\\alpha$ should be close to the test-set MSE of the unregularized solution, i.e. 708."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:06.723Z",
     "iopub.status.busy": "2020-05-28T07:37:06.694Z",
     "iopub.status.idle": "2020-05-28T07:37:07.241Z",
     "shell.execute_reply": "2020-05-28T07:37:07.508Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot of MSE  vs. alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which value of $\\alpha$ gives the minimum MSE? Is it better than the unregularized model? Why should the curve reach ~700 on the left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the same model using sklearn. Use the `linear_model.Ridge` object to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:07.315Z",
     "iopub.status.busy": "2020-05-28T07:37:07.282Z",
     "iopub.status.idle": "2020-05-28T07:37:07.361Z",
     "shell.execute_reply": "2020-05-28T07:37:07.552Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_regression_sklearn(X_test, X_train, y_train, alpha):\n",
    "    '''Computes OLS weights for regularized linear regression with regularization strength alpha using the sklearn\n",
    "       library on the training set and returns weights and testset predictions.\n",
    "\n",
    "       Inputs:\n",
    "         X_test: (n_observations, 81), numpy array with predictor values of the test set\n",
    "         X_train: (n_observations, 81), numpy array with predictor values of the training set\n",
    "         y_train: (n_observations,) numpy array with true target values for the training set\n",
    "         alpha: scalar, regularization strength\n",
    "\n",
    "       Outputs:\n",
    "         weights: The weight vector for the regerssion model including the offset\n",
    "         y_pred: The predictions on the TEST set\n",
    "\n",
    "       Note:\n",
    "         The sklearn library automatically takes care of adding a column for the offset.\n",
    "    '''\n",
    "\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    return weights, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, only plot how the performance changes as a function of $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:07.446Z",
     "iopub.status.busy": "2020-05-28T07:37:07.410Z",
     "iopub.status.idle": "2020-05-28T07:37:08.218Z",
     "shell.execute_reply": "2020-05-28T07:37:08.534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot of MSE  vs. alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Don't worry if the curve is not exactly identical to the one you got above. The loss function we wrote down in the lecture  has $\\alpha$ defined a bit differently compared to sklearn. However, qualitatively it should look the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Cross-validation\n",
    "\n",
    "Until now, we always estimated the error on the test set directly. However, we typically do not want to tune hyperparameters of our inference algorithms like $\\alpha$ on the test set, as this may lead to overfitting. Therefore, we tune them on the training set using cross-validation. As discussed in the lecture, the training data is here split in `n_folds`-ways, where each of the folds serves as a held-out dataset in turn and the model is always trained on the remaining data. Implement a function that performs cross-validation for the ridge regression parameter $\\alpha$. You can reuse functions written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:08.317Z",
     "iopub.status.busy": "2020-05-28T07:37:08.279Z",
     "iopub.status.idle": "2020-05-28T07:37:08.375Z",
     "shell.execute_reply": "2020-05-28T07:37:08.556Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridgeCV(X, y, n_folds, alphas):\n",
    "    '''Runs a n_fold-crossvalidation over the ridge regression parameter alpha.\n",
    "       The function should train the linear regression model for each fold on all values of alpha.\n",
    "\n",
    "      Inputs:\n",
    "        X: (n_obs, n_features) numpy array - predictor\n",
    "        y: (n_obs,) numpy array - target\n",
    "        n_folds: integer - number of CV folds\n",
    "        alphas: (n_parameters,) - regularization strength parameters to CV over\n",
    "\n",
    "      Outputs:\n",
    "        cv_results_mse: (n_folds, len(alphas)) numpy array, MSE for each cross-validation fold\n",
    "\n",
    "      Note:\n",
    "        Fix the seed for reproducibility.\n",
    "    '''\n",
    "\n",
    "    cv_results_mse = np.zeros((n_folds, len(alphas)))\n",
    "    np.random.seed(seed=2)\n",
    "\n",
    "    # ---------------- INSERT CODE ----------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------- END CODE -------------------------\n",
    "\n",
    "    return cv_results_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run 10-fold cross-validation using the training data of a range of $\\alpha$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:08.472Z",
     "iopub.status.busy": "2020-05-28T07:37:08.432Z",
     "iopub.status.idle": "2020-05-28T07:37:09.886Z",
     "shell.execute_reply": "2020-05-28T07:37:10.010Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-7, 7, 100)\n",
    "mse_cv = ridgeCV(X_train, y_train, n_folds=10, alphas=alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the MSE trace for each fold separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:09.963Z",
     "iopub.status.busy": "2020-05-28T07:37:09.933Z",
     "iopub.status.idle": "2020-05-28T07:37:10.445Z",
     "shell.execute_reply": "2020-05-28T07:37:10.539Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(alphas, mse_cv.T, '.-')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the average across folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:10.500Z",
     "iopub.status.busy": "2020-05-28T07:37:10.477Z",
     "iopub.status.idle": "2020-05-28T07:37:10.934Z",
     "shell.execute_reply": "2020-05-28T07:37:11.085Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(alphas, np.mean(mse_cv, axis=0), '.-')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the optimal $\\alpha$? Is it similar to the one found on the test set? Do the cross-validation MSE and the test-set MSE match well or differ strongly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run cross-validation on the full training data. This will take a moment, depending on the speed of your computer. Afterwards, we will again plot the mean CV curves for the full data set (blue) and the small data set (orange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:11.019Z",
     "iopub.status.busy": "2020-05-28T07:37:10.978Z",
     "iopub.status.idle": "2020-05-28T07:37:43.173Z",
     "shell.execute_reply": "2020-05-28T07:37:43.280Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-7, 7, 100)\n",
    "mse_cv_full = ridgeCV(X_train_full, y_train_full, n_folds=10, alphas=alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:43.249Z",
     "iopub.status.busy": "2020-05-28T07:37:43.218Z",
     "iopub.status.idle": "2020-05-28T07:37:43.744Z",
     "shell.execute_reply": "2020-05-28T07:37:43.860Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(alphas, np.mean(mse_cv_full, axis=0), '.-')\n",
    "plt.plot(alphas, np.mean(mse_cv, axis=0), '.-')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zoom in on the blue curve to the very left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T07:37:43.818Z",
     "iopub.status.busy": "2020-05-28T07:37:43.794Z",
     "iopub.status.idle": "2020-05-28T07:37:44.184Z",
     "shell.execute_reply": "2020-05-28T07:37:44.214Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(alphas, np.mean(mse_cv_full, axis=0), '.-')\n",
    "plt.xscale('log')\n",
    "minValue = np.min(np.mean(mse_cv_full, axis=0))\n",
    "plt.ylim([minValue-.01, minValue+.02])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the CV curve on the full data set look so different? What is the optimal value of $\\alpha$ and why is it so much smaller than on the small training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
